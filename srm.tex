\documentclass[11pt]{article}
\usepackage[letterpaper, left=.8in, top=0.9in, right=.8in, bottom=0.70in,nohead,includefoot, verbose, ignoremp]{geometry}
%\usepackage{charter} %choose default font ... your choice here % {mathptmx} % mathrsfs} % mathptmx} %mathpple} %mathpazo}
\usepackage[round]{natbib}
\usepackage{enumerate} % for different labels in numbered lists 
%\usepackage{xy}\xyoption{all} \xyoption{poly} \xyoption{knot}  
\usepackage{latexsym,amssymb,amsmath,amsfonts,graphicx,color,amsthm,enumerate,natbib,mathtools, bm} %,fancyvrb,movie15
\usepackage{dsfont}
\usepackage[pdftex,pagebackref=true]{hyperref}
\usepackage[svgnames,dvipsnames,x11names]{xcolor}
\hypersetup{
colorlinks,%
linkcolor=RoyalBlue2,  % colour of links to eqns, tables, sections, etc
urlcolor=Sienna4,   % colour of unboxed URLs
citecolor=RoyalBlue2  % colour of citations linked in text
}
\pagestyle{empty} % no page number on front page
\usepackage{todonotes}
%\usepackage{mathrsfs} % For \mathscr function

%\renewcommand{\includegraphics}{}  % use this to suppress inclusion of figs for proofing

% custom definitions ...
\def\eq#1{equation (\ref{#1})}
\def\pdf{p.d.f.\ } \def\cdf{c.d.f.\ }
\def\pdfs{p.d.f.s} \def\cdfs{c.d.f.s}
\def\mgf{m.g.f.\ } \def\mgfs{m.g.f.s\ }
\def\ci{\perp\!\!\!\perp}                        % conditional independence symbol
\def\beginmat{ \left( \begin{array} }
\def\endmat{ \end{array} \right) }
\def\diag{{\rm diag}}
\def\log{{\rm log}}
\def\tr{{\rm tr}}
\def\etr{{\rm etr}}
\def\Ei{{\rm Ei}}
\def\cD{\mathcal{D}}
\def\cS{\mathcal{S}}
\def\cM{\mathcal{M}}
%\newtheorem{thm}{Theorem}
\newcommand{\Reals}{\mathbb R}
\newcommand{\Beta}{\mathrm{Beta}}
\newcommand{\KL}{\mathrm{D}_{KL}}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newcommand{\df}{\vcentcolon=}
\newcommand{\ex}{{\mathbb E}}
\newcommand{\pfrac}[2]{\left(\frac{#1}{#2}\right)}

\DeclareBoldMathCommand{\balpha}{\alpha}
\DeclareBoldMathCommand{\bbeta}{\beta}
%

%%My Definitions
\def\qed{\hfill $\square$}
\newcommand{\indep}{\mathop{\perp\!\!\!\!\perp}}

\def\ts{\tilde{s}}

%% Document starts here ...
%%
\begin{document}
\vspace{-1in}
\title{SSRM: A Sequential Hypothesis Test for Compliance in Simply Randomized Experimental Designs}
\author{\Large Michael Lindon \\ Optimizely \and Alan Malek \\ Optimizely?}
\maketitle 
%\centerline{{\color{RoyalBlue2}{Due: someday.}}}\bigskip
%\thispagestyle{empty}
\begin{abstract}
  Simply randomized designs are one of the most common controlled experiments used to study causal effects.
  Failure of the assignment mechanism, to provide proper randomization of units across treatments, or the data collection mechanism, when data is ``missing not at random'', can render subsequent analysis invalid if not properly identified. In this paper we demonstrate that such practical implementation errors can often be identified, fortunately, through consideration of the total unit counts resulting in each treatment group.
  Based on this observation, we introduce a sequential hypothesis test constructed from Bayesian Multinomial-dirichlet families for detecting practical implementation errors in simply randomized experiments. By establishing a Martingale property of the posterior odds under the null hypothesis, frequentist Type-I error is controlled under both optional stopping and continuation via maximal inequalities, preventing practitioners from potentially inflating false positive probabilities through continuous monitoring.
  In contrast to other statistical tests which are performed once all data collection is completed, the proposed test is sequential - frequently rejecting the null during the process of data collection itself, saving further units from entering an improperly-executed experiment.
  We illustrate the utility of this test in the context of online controlled experiments (OCEs), where assignment is automated through code and data collected through complex processing pipelines, often in the presence of unintended bugs and logical errors. Confidence sequences posessing nominal sequential frequentist coverage probabilities are provided and their connection to the Bayesian support interval is examined. The differences between the pure Bayesian and sequential frequentist testing procedures are finally discussed through a conditional frequentist testing perspective.  
\end{abstract}


\section{Introduction}
Randomized treatment assignment satisfies many purposes in controlled experiments (see \cite{kempthorne}, \cite{cox} and \cite{rubin}). Arguably the least controversial justfication is the attempt to remove any personal, systematic or selection bias in the treatment assignment mechanism, although this is neither without criticism nor without alternative (see \cite{lindley} and \cite{kadane}). Consider for example a medical researcher who selects patients most likely to recover to receive her preferred experimental drug. Without explicitly conditioning on this information in the assignment mechanism, causal estimands such as the average treatment effect may be biased, overestimating the efficacy of the new drug (\cite{berry}).
Simply randomized experiments attempt to remove the possibility of bias by randomly assigning experimental units independently to treatment groups with a constant probability vector.
This design is useful in contexts where units enter the experiment sequentially, as opposed to being simultaneously available like in completely randomized designs, and are often used in the technology industry to run online controlled experiments (OCEs) (\cite{oce}). Formally let there be $d$ treamtent groups, $\theta \in \mathcal{S}^d$ be a probability vector where element $\theta_i$ denotes the probability of any unit being assigned to treatment group $i$, and $x_j$ a random variable denoting the assignment outcome of the $j'th$ experimental unit. The simply randomized design can then be summarized by the following probabilistic assignment mechanism
\begin{equation}
  \label{eq:multinomialassignment}
  x_1,x_2, \dots \sim \text{Multinomial}(1,\theta).
\end{equation}
The statistician may not be personally involved with the data collection process and may simply be told the assignment mechanism after being presented with the data for analysis. The observant statistican has a right for concern, therefore, when they are provided with data which does not support the purported assignment mechanism. For simply randomized experiments the total unit counts assigned to each treatment group can provide evidence against equation \eqref{eq:multinomialassignment}.
Indeed this is a strong indicator that the experiment has not been conducted as expected, either through a biased assignment mechanism depending on unreported covariates which the statistician cannot control for, or through an unintential missing data mechanism in the collection and reporting process, which could depend on the outcome being meaured such as censoring or other ``missing not at random'' (MNAR) mechanisms (\cite{missing-data}).
Such observations occurs frequently in OCEs and are colloquially referred to in the technology industry as \textit{sample ratio mismatches} (SRMs).

OCEs automate the assignment mechanism through code and instrument data collection and cleaning through complex transformtions which often introduce bugs and logical errors. It is unsurprising, therefore, that \cite{fabijan} reports that 6\% of all experiments performed in a at Microsoft contained bugs which were revealed by SRMs. The authors describe a simple experiment to study user engagement on an improved version of a web page. Not all visitors to a web page are human, however, and data must be cleaned to remove non-human interactions with the page, such as from web crawlers and scrapers. Unfortunately the classification between human and non-human visitors is performed algorithmically, and some users in the treatment group were so engaged with the new page that they were accidentally classified as non-human and removed prior to the analysis - essentially removing units most in favour of the treatment, resulting in fewer units than expected being reported in the treatment group. This is not an issue with the assignment mechanism, but is an example of a \textit{censoring} missing data mechanism, a special case of MNAR.

\cite{zhao} describes an example in which the user identifer becomes lost, preventing users from receiving a consistent experience over time, with some users initially assigned to the treatment becoming exposed to and recorded in the control - an example of noncompliance (\cite{imbens}). 

Many other practical abnormalities can be revealed by considering the total counts in each treatment group after collection. For this reason an industry best practise has evolved where practitioners run a Chi-squared test against the null in equation \eqref{eq:multinomialassignment}, after data collection has finished and prior to analysis, to test the validity of the collected data \cite{linkedin}.
The authors note two deficiencies of this check. The first deficiency, and most obvious, is that one learns about a problem in the data only after data collection has completed. It ia ssumed that there is an implicit cost of including a unit in an experiment, and so ideally one would like to learn about such problems as soon as possible to prevent further units from entering an improperly executed-experiment. The second deficiency, and motivated from the first, is that this encourages practitioners to incorrectly \textit{continuously monitor} their experiments through the repeated application of significance tests without any multiplicity correction (\cite{armitage}).

In order to develop a test that performs well under optional stopping and continuation the authors take inspiration from Bayesian methods satisfying the \textit{stopping rule principle}, that the reason for stopping an experiment should be irrelevant to the hypothesis being tested, which follows from the likelihood principle \cite{likelihood}. The contributions of this paper focus, however, on obtaining frequentist properties of such a sequential test.
The paper is outlined as follows. Section \ref{sec:srm_testing} defines a common Bayesian test through conjugate multinomial-Dirichlet models. Section \ref{sec:theory} establish the Martingale properties of the posterior odds under the null hypothesis, enabling a modified test to be developed which controls frequentist Type-I error probabilities below a nominal value under both optional stopping and continuation through maximal inequalities.  This safely permits the online testing of hypothesis \eqref{eq:multinomialassignment} after every single observation, without inflating frequentist false positive probabilities, with the obvious advantage of being able to safely reject the null and discover a practical implementation error early in the beginning of an experiment - preventing experimental units being wasted on a faulty experiment. Instance-specific upper-bounds on time-to-rejection are provided in terms of the KL divergence  between $\theta_0$ and the actual generating distribution of the samples. This sequential test is then inverted to define confidence sequences which possess nominal frequentist coverage probabilities. Section \ref{sec:simulation} presents a number of simulation studies illustrating how false positive probabilities are dramatically inflated through the repeated significance testing using a Chi-squared test compared to the guarantees afforded by the proposed test, in addition to how quickly the proposed test is able to reject the null when it is invalid. The final section \ref{sec:discussion} connects these contributions with existing literature. In particuar, the confidence sequence defined in \ref{sec:theory} is identified as the Bayesian support interval of \cite{support_interval} through an application of the Savage-Dickey density ratio. The differences between the pure Bayesian test and the proposed test are discussed in the context of recent work in conditional frequentist testing (\cite{conditional_frequentist_simple}, \cite{conditional_frequentist_precise}, \cite{conditional_frequentist_composite}).




%When a sample ratio mismatch is observed in an experiment and the difference is believed to be caused due to missing data, practitioners are neither willing to assume the missingness process is ignorable, nor are willing to model the missingness. The reluctance to model the missingness process is understandable because there are simply too many mechanisms by which data could be lost, which will be the focus of section \ref{causes_of_srms}, too many experiments being performed and OCEs are relatively cheap. It is far more common to test for an SRM and simply repeat the experiment after correcting the bug. Testing for SRMs is the focus of section \ref{srm_testing} 

\section{Conjugate Multinomial-Dirichlet Test}
\label{sec:srm_testing}
Let $x_1, x_2 ,\dots $ be a sequence of i.i.d. $\text{Multinomial}(1,\theta)$ random variables and consider the task of developing a sequential test for the simple hypothesis $\theta=\theta_0$. To do this, we consider the sequential properties of the Bayes factor resulting from the following model. Denote the null model as $M_0$ 
\begin{align}
    x_1, x_2, \dots &| M_0 \sim \text{Multinomial}(1,\theta_0),
\end{align}
and the alternative model as $M_1$
\begin{align}
  \theta &| M_1 \sim \text{Dirichlet}(\alpha),\\
  x_1, x_2, \dots &| \theta, M_1 \sim \text{Multinomial}(1,\theta).\notag
\end{align}
The Posterior odds in favour of $M_1$ to $M_0$ after observing $x_{1:t}$ is defined as
\begin{align}
  \label{eq:general_posterior_odds}
  \frac{p(M_1|x_{1:t})}{p(M_0|x_{1:t})}  &= \frac{\int p(x_{1:t}|\theta,M_1)p(\theta,M_1)d\theta}{p(x_{1:t}|M_0)}\frac{P(M_1)}{P(M_0)},\\
                      &=\frac{p(x_{1:t}|M_1)}{p(x_{1:t}|M_0)}\frac{p(M_1)}{p(M_0)},\\
                      &=\frac{\prod_{i=1}^{t}p(x_i|x_{1:i-1}|M_1)}{\prod_{i=1}^{t}p(x_i|x_{1:i-1}|M_0)}\frac{p(M_1)}{p(M_0)},\\
                      &=\frac{p(x_t|x_{1:t-1},M_1)}{p(x_t|x_{1:t-1},M_0)} \frac{p(M_1|x_{1:t-1})}{p(M_0|x_{1:t-1})},\\
    &=\frac{\int p(x_t|\theta,x_{1:t-1},M_1)p(\theta|x_{1:t-1},M_1)d\theta}{p(x_t|x_{1:t-1},M_0)}  \frac{p(M_1|x_{1:t-1})}{p(M_0|x_{1:t-1})} ,
\end{align}
where the last expression really stressess the recursive definition of the Bayes factor in terms of products of posterior predictive densities. The posterior distribution of $\theta| x_{1:t}, M_1 \sim \text{Dirichlet}(\alpha_t)$ where $\alpha_t = \alpha_{t-1}+x_t$ with $\alpha_0$ the initial prior parameter choice. The posterior predictive densities are easily computed as
\begin{equation}
  \label{eq:posterior_predictive_m1}
   p(x_t|x_{1:t-1},M_1) = \frac{ \Gamma(\sum_i x_{t,i}+ 1)}{\prod_i \Gamma(x_{t,i} + 1)} \frac{\Gamma(\sum_i \alpha_{t-1,i})}{\prod_i \Gamma(\alpha_{t-1,i})} \frac{\prod_i \Gamma(\alpha_{t-1,i} + x_{t,i})}{\Gamma(\sum_i \alpha_{t-1,i} + x_{t,i})},
\end{equation}
and
\begin{equation}
  \label{eq:posterior_predictive_m2}
   p(x_t|x_{1:t-1},M_0) = \frac{ \Gamma(\sum_i x_{t,i} + 1)}{\prod_i \Gamma(x_{t,i} + 1)} \prod p_i^{x_{t,i}}.
 \end{equation}
 It will be useful later on to introduce the following notation for the posterior odds at time $t$ as $O_t(\theta_0)$, which explicitly states the value of $\theta$ under the null hypothesis. The recursive definition of the posterior odds can then be expressed as
\begin{align}
  O_{t}(\theta_0) &= \frac{\Gamma(\sum_i \alpha_{t-1,i})}{\Gamma(\sum_i \alpha_{t-1,i} +  x_{t,i})} \frac{\prod_i \Gamma(\alpha_{t-1,i} + x_{t,i})}{\prod_i \Gamma(\alpha_{t-1,i})} \frac{1}{\prod_i \theta_{0,i}^{x_{t,i}}}  O_{t-1}(\theta_0),\\
\end{align}
with
\begin{align}
  \label{eq:alpha_update}
  \alpha_{t}&= \alpha_{t-1}+x_t.
\end{align}
and initial value
\begin{align}
  \label{eq:bayes_factor_seed}
O_0(\theta_0) = \frac{p(M_1)}{p(M_0)}.
\end{align}
Recall that for integer values $\Gamma(n)=(n-1)!$. In addition, $x_{t,i}$ is zero for all but one $i$. Let $j$ denote the index such that $x_{t,j}=1$. If $\alpha_0$ is integer valued, therefore, the recursive definition simplifies substantially to
\begin{align}
  \label{eq:simplified_bayes_factor}
  O_{t}(\theta_0) &= \frac{\alpha_{t-1,j}}{\sum_i \alpha_{t-1,i}} \frac{1}{\theta_{0,j}} O_{t-1}(\theta_0),\\
  &=\frac{E[\theta_j|x_{1:t-1}]}{\theta_{0,j}}  O_{t-1}(\theta_0),
\end{align}
where the last line follows from the mean of the Dirichlet posterior distribution. This multiplicative update has some intuitive appeal - it is the expected probability, based on our current Bayesian belief, divided by the null probability of the event that occured. We now consider properties of the sequence of posterior odds $\lbrace O_{i}(\theta_0) \rbrace_{i=1}^\infty $
\section{Obtaining a Frequentist Test}
\label{sec:theory}
\begin{thm}
  $O_i(\theta_0)$ as defined in equation \eqref{eq:general_posterior_odds} is a nonnegative margingale under $M_0$.
  \label{thm:posterior_odds_martingale}
    \end{thm}
  \begin{proof}
  \begin{align}
    E_{M_0}[O_{t+1}(\theta_0)|F_{t}]  &= \int \frac{p(x_{t+1}|x_{1:t},M_1)}{p(x_{t+1}|x_{1:t},M_0)} O_{t}(\theta_0) p(x_{t+1}|x_{1:t},M_0) d_{x_{t+1}}\\
    &=  O_{t}(\theta_0) \int p(x_{t+1}|x_{1:t},M_1) d_{x_{t+1}}\\
    &=  O_{t}(\theta_0).
  \end{align}
\end{proof}
In developing a test we might consider rejecting the null hypothesis as soon as the posterior odds in favour of the alternative exceed some tolerance. The following theorem establishes the probability of this event ever happening under the null, and hence the frequentist Type I error probability.
\begin{thm}
  \label{thm:type_1_error}
Let $x_1, x_2,\dots $ be a sequence of $\text{Multinomial}(1,\theta)$ random variables and consider the sequence $O_i(\theta_0)$ under the assumptions of theorem \ref{thm:posterior_odds_marginale}. Let $u >0$, then
\begin{equation}
  P_{\theta = \theta_0}\left( \sup_{i\in N} O_i(\theta_0) > 1/u \right) \leq u.
\end{equation}
\end{thm}
\begin{proof}
It follows from thorem \ref{thm:posterior_odds_martingale} that $O_i(\theta_0)$ is a nonnegative supermartingale under $M_0$ by definition. The result follows from an application of Ville's maximual inequality for nonnegative supermartingales. An alternative and self contained proof avoiding martingale theory is provided in \cite{robbins}. Yet another proof is provided by Wald's sequential probability ratio test for composite alternatives \cite{wald}.
\end{proof}
This suggests that if one continues to collect data while $O_i(\theta_0) \leq 1/u$ and rejects the null hypothesis as soon as $O_i(\theta_0) > 1/u$, then the probability of this happening under the null, resulting in a false positive, is at most $u$. Theorem \ref{thm:type_1_error} then suggests that this test is able to control the frequentist Type I error probability at a nominal level $\alpha$. This property is, for example, also shared by a trivial test which never rejects the null. In order for this sequential test to be compelling, it must also be able to reject the null with high probability when the null is not true.

\begin{thm} Let $\rho_t(\theta_0) = 1/(1+O_t(\theta_0))$ i.e. the posterior probability at time $t$ that the null hypothesis is correct. Under $M_0$ $E_{M_0}[\rho_{1+1}(\theta_0)|F_t] \geq \rho_{t}(\theta_0)$ i.e. the expected value of the posterior probability in favour of the null hypothesis is greater than or equal to the present value.
\end{thm}

\begin{proof}
  \begin{align}
      E_{M_0}\left[\frac{1-\rho_{t+1}(\theta_0)}{\rho_{t+1}(\theta_0)} | F_t \right] = \frac{1-\rho_{t}(\theta_0)}{\rho_{t}(\theta_0)},
  \end{align}
  by theorem \ref{thm:posterior_odds_martingale}. Yet $f(x)=(1-x)/x$ is a convex function, and so by Jensens inequality
    \begin{align}
      E_{M_0}\left[\frac{1-\rho_{t+1}(\theta_0)}{\rho_{t+1}(\theta_0)} | F_t \right] \geq \frac{1-E_{M_0}[\rho_{t+1}(\theta_0)|F_t]}{E_{M_0}[\rho_{t+1}(\theta_0)|F_t]}.
    \end{align}
    Combining these two conditions gives
    \begin{align}
       \frac{1-\rho_{t}(\theta_0)}{\rho_{t}(\theta_0)} \geq \frac{1-E_{M_0}[\rho_{t+1}(\theta_0)|F_t]}{E_{M_0}[\rho_{t+1}(\theta_0)|F_t]},
    \end{align}
    which implies $E_{M_0}[\rho_{t+1}(\theta_0)|F_t] \geq \rho_{t}(\theta_0)$ 
\end{proof}

I think the next theorem is necessary to demonstrate that this isn't a trivial test (never rejecting the null).
  \begin{thm}
  Asymptotically power 1. For any $p>0$
 \begin{equation} 
  P_{\theta \neq \theta_0}\left(B(x_{1:t};\theta_0) < 1/p \, \forall t \geq 1\right) = 0.
  \end{equation}
\end{thm}
\begin{proof}
  This is related to the consistency of Bayes factors. Consistency in this sense is that $B\rightarrow_p 0$ when $\theta=\theta_0$ and $B\rightarrow_p \infty$ when $\theta\neq \theta_0$ but contained in the support of $M_1$. There's a very in depth review paper by \cite{chib_bf_consistency}, but I think it builds more machinery than we need. This is a simple nested model, and it might be cleaner to use the construction in \cite{fractional_bf}, section 1.3 - asymptotics of Bayes factors. I think the proof here can be used to justify asymptotic power 1
\end{proof}


\begin{thm}
  For $p>0$ let $I_t = \lbrace \theta \in \mathcal{S}^d : B(x_{1:t}|\theta) \leq 1/p \rbrace $, then
  \begin{equation}
    \label{eq:always_valid_ci}
    P_{\theta}\left(\theta \in \bigcap_{t=1}^{\infty} I_t\right) > 1-p
  \end{equation}
\end{thm}
\begin{proof}
  This follows almost automatically from \ref{always_valid_p_value}. Yet there is more interesting discussion here. The Bayes factor can be expressed interstingly int erms of the Savage-Dickey density (\cite{dickey}) ratio i.e.
  \begin{equation}
    B(x_{1:t}|\theta_0) = \frac{p(\theta_0| M_1)}{p(\theta_0|x_{1:t},M_1)},
  \end{equation}
\end{proof}
This the ratio of posterior and prior probability densities under $M_1$ evaluated at the null parameter value. This means that the intervals $I_t$ can be expressed as
$I_t = \lbrace \theta \in \mathcal{S}^d : p(\theta_0| M_1)\leq p(\theta_0|x_{1:t}, M_1)/p \rbrace $, which is identically equal to the \textit{support interval} proposed in \cite{support_interval} (the authors have no idea this interval has frequentist coverage properties).

\section{Simulation Studies}
\label{sec:simulation}
\section{Indifference Region}

\begin{equation}
  A_n := \left\{ S_n : \frac{\Beta(\alpha_1 + S_n, \alpha_2 + n - S_n)}{\Beta(\alpha_1, \alpha_2)} \frac{1}{p_0^{S_n}(1-p_0)^{n-S_n}} \leq \frac{1}{\alpha} \right\}
\end{equation}

We want to show that this eventually degenerates to $\lbrace p_0 \rbrace$
\begin{align}
  \text{Beta}(x, y) &= \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}\\
  &=\frac{(x-1)!(y-1)!}{(x+y-1)!}
\end{align}
Consider Stirlings approximation
\begin{equation}
  \label{eq:stirling}
  x! = \sqrt{2\pi x} \left( \frac{x}{e} \right)^x   e^{\lambda_x},
\end{equation}
where $\lambda_x$ is $o(1)$ as $x\rightarrow \infty$. Let's just take $\alpha_1=\alpha_2=1$ for now, corresponding to a uniform prior
\begin{align*}
  \log \text{Beta}(S + 1, n - S + 1) =& \log S! + \log (n-S)! - \log n!-\log (n+1)\\
                         =&  \log \sqrt{2\pi S} + S \log S - S \log e  + \lambda_{S}\\
                         &+ \log \sqrt{2\pi (n-S)} +(n-S) \log (n-S) - (n-S) \log e  + \lambda_{n-S}\\
  &- \log \sqrt{2\pi n} - n\log n + n \log e  - \lambda_{n}\\
  &- \log (n+1)\\
  =&  \log \sqrt{2\pi\frac{ S(n-S)}{n}} + n H(S/n) - \log(n+1) + o(1)
\end{align*}.
The log posterior odds can then be approximated as
\begin{align*}
  \log O_n(\theta_0) =&  -S \log \theta_0 - (n-S)\log (1-\theta_0) + \log \sqrt{2\pi\frac{ S(n-S)}{n}} + n H(S/n) - \log(n+1) + o(1)\\
  =&  S\log(\frac{S/n}{\theta_0})  + (n-s) \log(\frac{1-S/n}{1-\theta_0})+ \log \sqrt{2\pi\frac{ S(n-S)}{n}} + - \log(n+1) + o(1)\\
     =&  n KL(\hat{\theta}|\theta_0) +  \log \sqrt{2\pi\frac{ S(n-S)}{n}} - \log(n+1) + o(1)\\
\end{align*}
as $n \rightarrow \infty$, where $\hat{\theta}=S/n$. Letting the test statistic be $\hat{\theta}=S/n$. The region of indifference can then be described as

\begin{equation}
  A_n := \lbrace S_n : KL(S/n|\theta_0)  \leq \frac{\log\left(\frac{1}{\alpha}\right)+\log(n+1) - \log \sqrt{2\pi\frac{ S(n-S)}{n}} + o(1)}{n} \rbrace
\end{equation}


\section{Multivariate Indifference Region}
Consider Stirlings approximation to the log factorial
\begin{equation}
  \label{eq:stirling_log_factorial}
  \log n! = \log\sqrt{2\pi n} + n \log n - n + \lambda_n,
\end{equation}
where $\lambda_n$ is $o(1)$ as $n\rightarrow \infty$. We can use this to approximate the multivariate Beta distribution. First lets consider a uniform distribution over the simplex i.e. $\alpha_i = 1$ for all $i$.
\begin{align*}
  \log \text{Beta}(s+\alpha) =& \sum_i \log \Gamma(s_i + 1) - \log \Gamma(n+d)\\
  =& \sum_i \log s_i! - \log (n+d-1)!\\
  =& \sum_i \left(\log\sqrt{2\pi s_i} + s_i \log s_i - s_i + \lambda_{s_i}\right)  \\
  &- \log\sqrt{2\pi (n+d-1)} - (n+d-1) \log (n+d-1) + (n+d-1) - \lambda_{n+d-1}\\
  =& \log\sqrt{(2\pi)^{d-1}\frac{ \prod_i s_i}{n+d-1}} +  \sum_i\left( s_i \log s_i + \lambda_{s_i}\right)  \\
       &- (n+d-1) \log (n+d-1) + (d-1) - \lambda_{n+d-1}\\
    =& \log\sqrt{(2\pi)^{d-1}\frac{ \prod_i s_i}{n+d-1}} +  \sum_i \left(n\frac{s_i}{n} \log \frac{s_i}n + \lambda_{s_i}\right)\\
       & + n\log n - (n+d-1) \log (n+d-1) + (d-1) - \lambda_{n+d-1}.
\end{align*}

The log posterior odds can then be approximated as
\begin{align*}
  \log O_n(\theta_0) =& \log\sqrt{(2\pi)^{d-1}\frac{ \prod_i s_i}{n+d-1}} +  \sum_i n\frac{s_i}{n} \log \frac{s_i}n -  \sum_i n\frac{s_i}{n} \log \theta_{0i} + \lambda_{s_i},  \\
                      & + n\log n - (n+d-1) \log (n+d-1) + (d-1) - \lambda_n,\\
   =& nKL(\hat{\theta}|\theta_0)+ \log\sqrt{(2\pi)^{d-1}\frac{ \prod_i s_i}{n+d-1}}  + n\log n - (n+d-1) \log (n+d-1) + (d-1)+o(1)\\
\end{align*}
Once again let $\hat{\theta}(x_1,...)$ form our test statistic, then the indifference region can be written as

\begin{equation}
  A_n := \lbrace \hat{\theta}(x) \in  S^d : KL(\hat{\theta}|\theta_0)  \leq \frac{\log\left(\frac{1}{\alpha}\right) - \log\sqrt{(2\pi)^{d-1}\frac{ \prod_i s_i}{n+d-1}}+ (n+d-1) \log (n+d-1) -  n \log n -(d-1)+o(1) }{n} \rbrace
\end{equation}
The right-hand side of the inequality is $o(1)$ i.e.
\begin{equation}
  A_n := \lbrace \hat{\theta}(x) \in  S^d : KL(\hat{\theta}|\theta_0)  \leq g(n) \rbrace,
\end{equation}
where $g(n)$ is $o(1)$.
Let $A_{\infty} = \bigcap_{n=1}^{\infty} A_n$, then $A_{\infty} = \lbrace \theta_0 \rbrace$. First note that $\theta_0 \in A_n \, \forall n$ because $KL(\theta_0|\theta_0) = 0$. For $\theta \neq \theta_0$ we have that $KL(\theta|\theta_0) = \varepsilon >  0$, therefore there exists an $N>0$ s.t. $g(n) < \varepsilon$ for all $n \geq N$ and so $\theta \neq A_n$ for all $n \geq N$.



\section{Bounding the time to significance}
The false positive guarantees the validity of the sequential SRM test, but they are only half of the equation. This section established good performance on the other side; namely, that the SSRM will find true differences quickly. In particular, we provide high probability bounds on the stopping time for data generated from some true alternative distribution $\theta_1$ in terms of some TBD distance measure of $\theta_1 - \theta_0$.

For the general multinomial case, we will use bold greek letters $\balpha = (\alpha_1,\ldots, \alpha_d)$ for vectors (in particular, distinguishing from the scalar $\alpha$); we will also use $\balpha^\bbeta = \prod_{i}^d \alpha_i^{\beta_i}$ to denote elementwise exponentiation, and we will use $|\balpha| = \sum_i\alpha_i$ for the sum. For the vector of counts, we will write $S_n=(S_1^n,\ldots, S_d^n)\in\Reals^d$, which lets of easily define the empirical probability of outcome $i$ as $\hat \theta_i^n \df S_n^i/n$. For any $\balpha,\bbeta\in\Reals^d$, we interpret exponentiation to be elementwise and define 

For convenience, we will also define $\Beta(\bbeta) \df \prod_i \Gamma(\beta_i) / \Gamma(\sum_i\beta_i)$ for any vector $\bbeta$. The test rejects the null hypothesis corresponding to $\theta_0$ when the event
\begin{equation}\label{eq:multinomial.rejection.region}
  \mathcal E_n \df \left\{\frac{\Beta(\balpha + S_n)}
  {\Beta(\balpha)}
  \theta_0 ^{-S_n} \geq \frac{1}{\alpha}\right\}
\end{equation}
occurs.

Our goal is to provide a guarantee of the form: if $S^n\sim\mathrm{multinomial}(\theta_1,n)$, then with probability $\delta>0$, there exists an $N$ such that 
\begin{equation*}
  P \left(\forall n \geq N,\;
      \frac{\Beta(\balpha + S_n)}
  {\Beta(\balpha)}
  \theta_0 ^{-S_n} \geq \frac{1}{\alpha}
  \right) \geq 1-\delta.
\end{equation*}
We would also like to upper bound this $N$ as a function of $\theta_1$ and $\theta_0$.

We will accomplish this goal in two parts. First, we will lower bound 
$
  P \left(
      \frac{\Beta(\balpha + S_n)}
  {\Beta(\balpha)}
  \theta_0 ^{-S_n} \geq \frac{1}{\alpha}
  \right) 
$
as a function of $S_n$. Then, we will construct a confidence sequence on $S_n$ (in terms of $n$ and $\theta_1$) and combine the two.

\subsection{Manipulating the rejection probability}
Our goal is to find some event $\mathcal E_n'\subseteq \mathcal E_n$ that is more amenable to computation; this allows us to obtain a bound on the power since
$P(\forall n \geq N,\; \mathcal E_n) \geq P(\forall n \geq N,\; \mathcal E_n')$.  Specifically, we will prove the following theorem.
\begin{thm}\label{thm:calEprime}
Define, for every $n>0$ and $\alpha, S_n\in(\Reals^+)^d$, the set
\begin{equation}
  \label{eq:decision_boundary}
  \mathcal E_n'\df
  \left\{
    \KL(\hat\theta_n|| \theta)
    \geq
      \frac{|\balpha|^2}{n^2} + \frac{|\balpha|}{n}
    +\frac{1}{n}\log\frac{\Beta(\balpha)}{\alpha}
  \right\}.
\end{equation}
Then $\mathcal E_n' \subseteq \mathcal E_n$ for all $n>0$. 
\end{thm}
This $\mathcal E_n'$ is much more convenient. For example, if $\alpha_i = 1$ for all $i$, then 
\begin{equation*}
  \mathcal E_n'\df
  \left\{
    \KL(\hat\theta_n|| \theta)
    \geq
    \frac{d^2}{n^2}
    +\frac{d}{n}
    +\frac{1}{n}\log\frac{1}{\alpha}
  \right\}.
\end{equation*}

The proof is by explicit construction, and we begin by deriving a lower bound on 
$
 \frac{\Beta(\balpha + S_n)} {\Beta(\balpha)} \theta_0 ^{-S_n}
$.
There are many ways to bound the Beta function; for the two dimensional case, it suffices to use
\begin{equation}\label{eq:Beta.lower.bound}
  \Beta(x,y) \geq \frac{x^{x-1}y^{y-1}}
  {(x+y)^{x+y-1}}\quad\forall x,y>0,
\end{equation}
borrowed from \cite{grenie2015inequalities}. We will need the following multinomial generalization.
\begin{lemma}\label{lem:beta.lower.bound}
  For all $\balpha$ in the positive orthant,
  \begin{equation*}
    \Beta(\balpha) \geq
    \frac{ \prod_i \alpha_i^{\alpha_i-1}}
    {(\sum_i \alpha_i)^{\sum_i \alpha_i-1}}.
  \end{equation*}
\end{lemma}
\begin{proof}
  We mostly follow the derivation of equation (4) in \cite{grenie2015inequalities}. For any completely monotone function $f:(0,\inf)\rightarrow(0,1]$, a result from \cite{kimberling1974probabilistic} yields that
\[
  \frac{f(x+y)}{f(x)f(y)} \geq 1 \; \forall x,y>0,
\]
Applying this identity repeatedly, we see that
\[
  f(|\balpha|)
  \geq
  f(\alpha_1)f\left(\textstyle \sum_{i>1} \alpha_i\right)
  \geq \ldots\geq
  \prod_i f(\alpha_i).
\]
We will apply this identity to the completely monotone function $\exp(-H(x))$, where
\[
  H(x) = x - x\log(x) + \log\Gamma(x+1);
\]
see \cite{grenie2015inequalities} for a justification. This produces
\begin{align*}
  \frac{1}{\Gamma(|\balpha|+1)}e^{-|\balpha|+|\balpha|\log(|\balpha|)}
  &=
  e^{-H\left(|\balpha|\right)}\\
  &\geq
    e^{-\sum_i H\left(\alpha_i\right)}\\
  &=
     e^{\sum_i\left(- \alpha_i + \alpha_i \log(\alpha_i)\right)}\prod_i\frac{1}{\Gamma(\alpha_i+1)}.
\end{align*}
Rearranging yields
\[
  \frac{\prod_i\Gamma(\alpha_i+1)}{\Gamma(|\balpha|+1)}
  \geq
  \frac{\prod_i \alpha_i^{\alpha_i}e^{-\alpha_i}}
  {|\balpha|^{|\balpha|}e^{-|\balpha|}},
\]
which implies the bound
\begin{align*}
  \Beta(\balpha)
  =
  \frac{\prod_i\Gamma(\alpha_i)}{\Gamma(|\balpha|)}
  &=
  \frac{(|\balpha|+1)\prod_i}
  {\prod_i (\alpha_i+1)}
   \frac{\Gamma(\alpha_i+1)}{\Gamma(|\balpha|+1)}\\
  &\geq
  \frac{\prod_i \alpha_i^{\alpha_i}}
  {|\balpha|^{|\balpha|}}
  \frac{(|\balpha|+1)}{\prod_i (\alpha_i+1)}\\
  &\geq
  \frac{\prod_i \alpha_i^{\alpha_i-1}}
  {|\balpha|^{|\balpha|-1}}.
\end{align*}
\end{proof}
We can now finish the proof of Theorem~\ref{thm:calEprime}.
\begin{proof}
Expanding the left hand side of \eqref{eq:multinomial.rejection.region}, and applying Lemma~\ref{lem:beta.lower.bound}, we see
\begin{align*}
    \frac{\Beta(\balpha + S_n)}
  {\Beta(\balpha)} \theta_i^{-S_n}
  &\geq
    \frac{ \prod_i (\alpha_i+S_i^n)^{\alpha_i + S_i^n-1}}
    {(|\balpha|+n)^{|\balpha| + n - 1}}
    \frac{ \theta_i^{-S_n}}{\Beta(\balpha)}\\
  &=
    n^n
    \prod_i
    \left(
    \frac{\frac{\alpha_i}{n} + \hat \theta_i^n}{\theta_i}\right)^{S_i^n}
    \frac{
    \prod_i
    (\alpha_i + S_i^n)^{\alpha_i - 1}
    }
    {(|\balpha|+n)^{|\balpha|+n-1}\Beta(\balpha)}\\
  &\geq
    n^n
    \prod_i
    \left(
    \frac{\hat \theta_i^n}{\theta_i}\right)^{S_i^n}
    \frac{
    \prod_i
    (\alpha_i + S_i^n)^{\alpha_i - 1}
    }
    {(|\balpha|+n)^{|\balpha|+n-1}\Beta(\balpha)}\\
  &=
    n^n
    e^{n \KL(\hat\theta_n|| \theta) }
    \frac{
    \prod_i
    (\alpha_i + S_i^n)^{\alpha_i - 1}
    }
    {(|\balpha|+n)^{|\balpha|+n-1}\Beta(\balpha)}.
\end{align*}
This lets us construct
\begin{align*}
  \mathcal E_n
  &=
  \left\{
    \log \left(
    \Beta(\balpha+S_n) \theta_i^{-S_n}
  \right)
  \geq \log\frac{\Beta(\balpha)}{\alpha}
    \right\}\\
  &\supseteq
    \left\{
        n\log(n)
    +n \KL(\hat\theta_n|| \theta)
    +
    \log\left(\frac{
    \prod_i
    (\alpha_i + S_i^n)^{\alpha_i - 1}
    }
    {(|\balpha|+n)^{|\balpha|+n-1}}\right)
    \geq
    \log\frac{\Beta(\balpha)}{\alpha}
    \right\}\\
  &=
    \left\{
    n \KL(\hat\theta_n|| \theta)
    +
    \sum_i\log\left(
    (\alpha_i + S_i^n)^{\alpha_i - 1}
    \right)
    \geq
    (|\balpha|+n-1)\log(|\balpha|+n)
    -n\log(n)
    +\log\frac{\Beta(\balpha)}{\alpha}
    \right\}\\
    &\supset
    \left\{
    n \KL(\hat\theta_n|| \theta)
    +
    \sum_i\log\left(
    (\alpha_i + S_i^n)^{\alpha_i - 1}
    \right)
    \geq
      \frac{|\balpha|^2}{n} + |\balpha|
    +\log\frac{\Beta(\balpha)}{\alpha}
    \right\},
\end{align*}
where the last line followed because, under the assumption that $|\balpha| \geq 1$
\begin{align*}
    (|\balpha|+n-1)\log(|\balpha|+n)
  -n\log(n)
  &\geq
    (|\balpha|+n-1)\log(|\balpha|+n)
    -(|\balpha|+n-1)\log(n)\\
  &\geq
  (|\balpha|+n-1)
    \log\left(1+\frac{|\balpha|}{n}\right)\\
  &\geq
    (|\balpha|+n-1)\frac{|\balpha|}{n}
    \geq
    \frac{|\balpha|^2}{n} + |\balpha|.
\end{align*}

The last step is to upper-bound
We can upper bound the final remaining summation by
\begin{align*}
  \sum_i(\alpha_i-1)\log
  \left(
  \alpha_i + S_i^n
  \right)
  &\leq
    (|\balpha|-d)\log
    \left(
    |\balpha|+n
    \right),
\end{align*}
showing that
\[
      \left\{
    n \KL(\hat\theta_n|| \theta)
    \geq
      \frac{|\balpha|^2}{n} + |\balpha|
      +\log\frac{\Beta(\balpha)}{\alpha}
      -
          (|\balpha|-d)\log
    \left(
    |\balpha|+n
    \right)
  \right\}
  \subseteq \mathcal E_n,
\]
which implies that $\mathcal E_n' \subseteq \mathcal E_n$ (the bound on the summation is only included for completeness).
\end{proof}

\subsection{Deriving a Confidence Sequence for $S_n$}
The next step in the argument is to derive a confidence sequence for $S_n$ under some alternative hypothesis. In fact, we only need to derive a lower bound on $\KL (\hat\theta_n||\theta_0)$ in order to apply the results of the previous section.

In particular, we will prove a confidence sequence on the total variation norm directly. This confidence sequence is novel and may be of independent interest.
\begin{lemma}\label{lem:CS}
  Fix some $\theta$ in the $d$-simplex and assume that $X_t\sim\mathrm{Multinomial}(\theta)$ for $t>0$. Let $\hat\theta_n$ be the empirical frequencies after $n$ samples. For any $\delta>0$,  we have
\begin{equation}\label{eqn:multinomial.CS}
  P\left( \exists n > 0: \Vert \hat\theta_n - \theta\Vert_1
    \geq
  \frac{2}{n}\sqrt{\left(\frac{n}{4} + \rho\right)\log\pfrac{n/4+\rho}{2^{-2d}\delta^2 \rho}} 
  \right)
  \leq \delta.
\end{equation}  
\end{lemma}
\begin{proof}
  We will follow the construction of \cite{weissman2003inequalities}.
  Using the shorthand $\theta'(A) \df P(Z\in  A)$ where $Z\sim\mathrm{Multinomial}(\theta')$, the total variation and $L_1$ norms are related by
\[
  \Vert \hat\theta_t - \theta\Vert_1
  =
  2 \max_{A \subseteq \{1,\ldots, d\}} \hat\theta_t(A) - \theta(A),
\]
and so we may use a union bound to arrive at
\begin{align*}
  P(\exists n > 0:\; \Vert \hat\theta_n - \theta\Vert_1 \geq \epsilon)
  \leq
  \sum_{A \subset \{1,\ldots, d\}}
  P\left(\exists n > 0:\; \hat\theta_n(A) - \theta(A) \geq \frac{\epsilon}{2}\right).
\end{align*}

Hence, for any $A \subset \{1,\ldots, d\}$, we want to upper bound
\[
 P\left(\exists n > 0:\; \hat\theta_n(A) - \theta(A) \geq \frac{\epsilon}{2}\right).
\]

Fortunately, $n \hat\theta_n(A)$ is the sum of independent Bernoulli random variables $\mathds{1}\{ \Delta S_n \in A\}$ with mean $\theta(A)$ (where $\mathds{1}\{\cdot\}$ is the indicator function). This implies that $n \hat\theta_n(A)$ is the sum of sub-Gaussian random variables with $\sigma^2 = 1/4$, and we may apply any sub-Gaussian mixture boundary. In particular, \cite[Equation~14]{howard2018uniform} implies that
\[
  P\left(\exists n > 0: \hat\theta_n(A) - \theta(A) \geq
    \frac{1}{n}\sqrt{\left(\frac{n}{4} + \rho\right)\log\pfrac{n/4+\rho}{(\delta')^2 \rho}}
  \right)
  \leq
  \delta',
\]
where $\rho$ is mixture variance.
Hence, choosing $\delta' = 2^{-d} \delta$, we have
\begin{align*}
  \lefteqn{
  P\left(\exists n > 0:\; \Vert \hat\theta_n - \theta\Vert_1 \geq
  \frac{2}{n}\sqrt{\left(\frac{n}{4} + \rho\right)\log\pfrac{n/4+\rho}{2^{-2d}\delta^2 \rho}} 
\right)
  }\\
  &\leq
  \sum_{A \subset \{1,\ldots, d\}}
  P\left(\exists n > 0:\; \hat\theta_n(A) - \theta(A)
  \geq
    \frac{1}{n}\sqrt{\left(\frac{n}{4} + \rho\right)\log\pfrac{n/4+\rho}{2^{-2d}\delta^2 \rho}}  
  \right)\\
  &\leq
    \sum_{A \subset \{1,\ldots, d\}}
    \delta 2^{-d}\\
  &\leq
  \delta.
\end{align*}



\end{proof}

We can now combine the two lemmas of this section into a finite-sample power guarantee. We will denote the decision boundary from \eqref{eq:decision_boundary} by
\[\Gamma_n(\balpha)
  \df
  \frac{|\balpha|^2}{n^2} + \frac{|\balpha|}{n}
  +
  \frac{1}{n}\log\frac{\Beta(\balpha)}{\alpha}.
\]
\begin{thm}\label{thm:power_lower_bound}
  Assume that $S_n$ has a multinomial $\tilde\theta$ distribution. For every $\balpha$, $\delta$, $\alpha$, $\tilde\theta$, and $\theta_0$, there exists an $N$ such that
  \begin{equation*}
    P\left(\forall n \geq N: \KL(\hat\theta_n \Vert \theta_0) \geq \Gamma_n(\balpha)\right)
    \geq 1-\delta.
  \end{equation*}
\end{thm}
\begin{proof}
  Define the event
  \[
    \mathcal R_n
    \df
    \left\{
    \KL(\hat\theta_n \Vert \theta_0) \geq \Gamma_n(\balpha)
  \right\}.
\]
By Applying Pinsker's inequality then the triangle inequality, we can show that
\[
  \mathcal R_n \supseteq
  \left\{
    \Vert \hat\theta_n - \theta_0\Vert_1 \geq \sqrt{2\Gamma_n(\balpha)}
  \right\}
  \supseteq
    \left\{
      \Vert \tilde\theta - \theta_0\Vert_1
      -
      \Vert \hat\theta_n - \tilde\theta\Vert_1
      \geq \sqrt{2\Gamma_n(\balpha)}
  \right\}.
\]
Hence, we would like to find an $N$ such that
\[
      P\left(
      \Vert \tilde\theta - \theta_0\Vert_1
      -
      \Vert \hat\theta_n - \tilde\theta\Vert_1
      \geq \sqrt{2\Gamma_n(\balpha)}
  \right)\geq 1-\delta
\]
for all $n\geq N$. By Lemma~\ref{lem:CS}, with probability at least $1-\delta$, the event
\begin{align*}
  \left\{ \forall n > 0:
  \Vert\hat\theta_n- \tilde\theta\Vert_1
  \leq
    \frac{2}{n}\sqrt{\left(\frac{n}{4} + \rho\right)\log\pfrac{n/4+\rho}{2^{-2d}\delta^2 \rho}}\right\}
\end{align*}
occurs.

The existence of an $N$ that satisfied the conditions of the theorem can be show by choosing any $N$ that satisfies
\[
  \Vert \theta_0- \tilde\theta\Vert_{1}
    \geq
        \frac{2}{n}\sqrt{\left(\frac{n}{4} + \rho\right)\log\pfrac{n/4+\rho}{2^{-2d}\delta^2 \rho}}
    + \sqrt{2\Gamma_n(\balpha)}.
  \]
Such an $N$ is well defined because the left hand side is a constant and the right hand side is a decreasing fuction of $N$. In particular, we may simply take $\gamma = 1$; the right most term decays like $1/\sqrt{n}$, so optimizing $\gamma$ will not yield a faster rate. 


Combining the previous few steps, we have
\begin{equation*}
    P\left(\forall n>0 \leq N: \KL(\hat\theta_n \Vert \theta_0) \geq \Gamma_n(\balpha)\right)
    \geq 1-\delta,
  \end{equation*}
  completing the proof.
\end{proof}
\begin{lemma}
  For the $N$ defined by Theorem~\ref{thm:power_lower_bound}, we have
  \begin{equation*}
    N = \Omega\left(
      \frac{d}{\Vert \theta_0- \tilde\theta\Vert_{1}^2}
\log\pfrac{1}{\delta}\log\pfrac{1}{\alpha}
    \right).
  \end{equation*}
\end{lemma}
\begin{proof}
The sample complexity will be the solution to
\[
  N \df
  \min
  \left\{
    n:\;
      \left(1 + \frac{4\rho}{n}\right)
      \log\pfrac{n/4+\rho}{2^{-2d}\delta^2 \rho}
    +
      \frac{2|\balpha|^2}{n^2} + \frac{2|\balpha|}{n}
      +
    \frac{2}{n}\log\frac{\Beta(\balpha)}{\alpha}
    \leq
    \frac{1}{2}\Vert \theta_0- \tilde\theta\Vert_{1}^2
  \right\},
\]
  which has the same order as described in the lemma. To check that this $N$ is sufficient, we will argue that this $N$ ensures that
  \begin{equation}\label{eqn:n.bound}
    \Vert \theta_0- \tilde\theta\Vert_{1}
    \geq
    \frac{2}{n}\sqrt{\left(\frac{n}{4} + \rho\right)
      \log\pfrac{n/4+\rho}{2^{-2d}\delta^2 \rho}}
    + \sqrt{2}\sqrt{
      \frac{|\balpha|^2}{n^2} + \frac{|\balpha|}{n}
      +
      \frac{1}{n}\log\frac{\Beta(\balpha)}{\alpha}
    }.
\end{equation}  

Since convexity of the square root implies  $\sqrt{a}+\sqrt{b}\leq\sqrt{2(a+b)}$, we can show that
\begin{align*}
    \lefteqn{\frac{2}{n}\sqrt{\left(\frac{n}{4} + \rho\right)
      \log\pfrac{n/4+\rho}{2^{-2d}\delta^2 \rho}}
    + \sqrt{2}\sqrt{
      \frac{|\balpha|^2}{n^2} + \frac{|\balpha|}{n}
      +
      \frac{1}{n}\log\frac{\Beta(\balpha)}{\alpha}
  }}\\
  &\leq
    \sqrt{2}
      \sqrt{\frac{1}{n}\left(1 + \frac{4\rho}{n}\right)
      \log\pfrac{n/4+\rho}{2^{-2d}\delta^2 \rho}
    +
      \frac{2|\balpha|^2}{n^2} + \frac{2|\balpha|}{n}
      +
    \frac{2}{n}\log\frac{\Beta(\balpha)}{\alpha}
    },
\end{align*}
which implies that, under $n \geq N$, we have
\begin{align*}
    \Vert \theta_0- \tilde\theta\Vert_{1}
  &\geq
    \sqrt{
    \left(1 + \frac{4\rho}{n}\right)
      \log\pfrac{n/4+\rho}{2^{-2d}\delta^2 \rho}
    +
      \frac{2|\balpha|^2}{n^2} + \frac{2|\balpha|}{n}
      +
    \frac{2}{n}\log\frac{\Beta(\balpha)}{\alpha}
    }\\
    &\geq
      \frac{2}{n}\sqrt{\left(\frac{n}{4} + \rho\right)
      \log\pfrac{n/4+\rho}{2^{-2d}\delta^2 \rho}}
    + \sqrt{2}\sqrt{
      \frac{|\balpha|^2}{n^2} + \frac{|\balpha|}{n}
      +
      \frac{1}{n}\log\frac{\Beta(\balpha)}{\alpha}
      }.
\end{align*}
\end{proof}

\subsection{Extensions}
We would like to choose $\gamma$ in Lemma~\ref{lem:CS} to minimize the upper bound. This means that we would like
For now, assume that we chose $\gamma$ to be the minimizer of the bound
\[
  \gamma^* = \sqrt{ 2\frac{d}{n \Vert \tilde\theta\Vert_2^2}\log\left(\frac{2d}{\delta}\right)},
\]
which guarantees
\begin{align*}
P  \left( \forall n > 0: \Vert\hat\theta - \tilde\theta\Vert_{TV}
  \leq
  \sqrt{ 8\frac{d}{n \Vert \tilde\theta\Vert_2^2}\log\left(\frac{2d}{\delta}\right)}
  \right) \geq 1-\alpha.
\end{align*}
Unfortunately, the $\gamma$ must be chosen ahead of time. We may approximate this by taking a mixture boundary, where we integrate $\gamma$ against some mixture distribution. 

\bibliographystyle{plainnat}
\bibliography{sample}


\appendix

\section{Trying to use a sub-Bernoulli boundary}
The difficulty in trying to use the CGF of the Bernoulli random variables directly is that the typical Chernoff method applied to Bernoullis provide a bound of the form
\[
  \P(\hat\theta_n > (1+\delta)\theta)\leq \exp(-\mu\delta^2/3).
\]
To get an additive bound needed by the rest of the argument, we would need to tune $\lambda$ as a function of $n$. Maybe this goal can be accomplished by the method of mixtures. I've included some partial derivations below in case they come in handy.

Fortunately, $n \hat\theta_n(A)$ is the sum of independent Bernoulli random variables $\mathds{1}\{ \Delta S_n \in A\}$ with mean $\theta(A)$ (where $\mathds{1}\{\cdot\}$ is the indicator function). The cumulant generating function of a Bernoulli random variable is $\psi(\lambda) = \log\left(1 - \theta(1-e^\lambda)\right)$, and so
\[
  P\left(
    \exists n > 0:\;
    \lambda \hat\theta_n(A)
    \geq
    \frac{1}{n}\log\left(\frac{1}{\delta}\right)
    + \log\left(1 - \theta(A)(1-e^\lambda)\right)
  \right)
  \leq \delta.
\]
We can upper bound the cumulant generating function by
\[
  \log\left(1 - \theta(A)(1-e^\lambda)\right)
  \leq
  \theta(A)(e^\lambda-1)
  \leq
  \theta(A)(\lambda+\lambda^2)
\]
where the second inequality holds  on $\lambda \leq 1.79$. Thus, we can show that 
\begin{align*}
  \delta
  &\geq 
    P\left(
    \exists n > 0:\;
    \lambda \hat\theta_n(A)
    \geq
    \frac{1}{n}\log\left(\frac{1}{\delta}\right)
    + \log\left(1 - \theta(A)(1-e^\lambda)\right)
    \right)\\
  &\geq
    P\left(
    \exists n > 0:\;
    \lambda \hat\theta_n(A)
    \geq
    \frac{1}{n}\log\left(\frac{1}{\delta}\right)
    +  \theta(A)(\lambda + \lambda^2)
    \right)\\
  &\geq
    P\left(
    \exists n > 0:\;
    \hat\theta_n(A) - \theta(A)
    \geq
    \frac{1}{\lambda n}\log\left(\frac{1}{\delta}\right)
    +  \theta(A)\lambda
    \right)\\
    &\geq
    P\left(
    \exists n > 0:\;
    \hat\theta_n(A) - \theta(A)
    \geq
    \frac{1}{\lambda n}\log\left(\frac{1}{\delta}\right)
    +  \lambda
    \right)    
\end{align*}
While it might be tempting to optimizintg over $\lambda$ and obtain a $1/\sqrt{n}$ rate, we cannot choose $\lambda$ as a function of $n$ or of the data. A technique such as the method of mixtures \citep{delapena2000moment} would be required. Fortunately, for our case, it suffices to take $\lambda = 1$.

Finally, we set  $\epsilon = (\frac{1}{n}\log(\frac{2^d}{\delta}) + 1)$ and assemble these ingredients together to find
\begin{align*}
  P(\exists n > 0:\; \Vert \hat\theta_n - \theta\Vert_1 \geq \epsilon)
  &\leq
  \sum_{A \subset \{1,\ldots, d\}}
  P\left(\exists n > 0:\; \hat\theta_n(A) - \theta(A) \geq \frac{\epsilon}{2}\right)\\
  &\leq
  \sum_{A \subset \{1,\ldots, d\}}
  P\left(\exists n > 0:\; \hat\theta_n(A) - \theta(A) \geq
    \frac{1}{t}\log\left(\frac{2^d}{\delta}\right) + 1
    \right)\\
  &\leq
  \sum_{A \subset \{1,\ldots, d\}}
    \delta 2^{-d}\\
  & =
    \delta.
\end{align*}

\section{Michael: Using Concentration Inequality}
From the previous sections we note that the \textbf{indifference region}, where
the null hypothesis is not rejected, is
\begin{align}
  I_n = \lbrace x \in S^d : D_{KL}(x||\theta_0) \leq k_n \rbrace,
\end{align}
where $k_n$ is just some constant as a function of $n$. We also note the concentration inequality
\begin{align}
  \mathbb{P}[\hat{\theta}_n \in C_n^{\delta}] \geq 1-\delta,
\end{align}
where
\begin{align}
  C_n^{\delta}=\lbrace x \in S^d : \|x-\theta\|_1 \leq \sqrt{\frac{4\log \frac{2}{\delta}}{n}}\rbrace.
\end{align}
From this we know that with high probability the MLE $\hat{\theta}_n \in C_n^{\delta}$ and, if we can show that $C_n^{\delta} \cap I_n = \emptyset$, then with high probability the null hypothesis is rejected. As a first step lets create a superset of $I_n$ that is defined in terms of the L1 norm instead of the Kullback Leibler divergence. From Pinsker's inequality $2\|x-\theta_0\|_1 \leq D_{KL}(x||\theta_0)$ and so we may define a set
\begin{align}
  J_n = \lbrace x \in S^d : \|x-\theta_0\|_1 \leq \frac{1}{2}k_n \rbrace.
\end{align}
If $x$ satisfies $D_{KL}(x||\theta_0) \leq k_n$, then by Pinsker's inequality $\|x-\theta_0\|_1 \leq \frac{1}{2}k_n$ is satisfied also, and so $I_n \subset J_n$. The next step is to choose a $\delta$ such that $C_n^\delta \cap  J_n = \emptyset$. From an application of the reverse triangle inequality it follows that
\begin{align*}
  C_n^\delta \subset D_n^\delta =& \lbrace x \in S^d : |\|x-\theta_0\|_1 - \|\theta-\theta_0\|_1 | \leq \sqrt{\frac{4\log \frac{2}{\delta}}{n}}\rbrace\\
  &\subset \lbrace x \in S^d : \|\theta-\theta_0\|_1  - \sqrt{\frac{4\log \frac{2}{\delta}}{n}} \leq  \|x-\theta_0\|_1  \rbrace := E_n^{\delta}.
\end{align*}
Let's choose $\delta$ to be a function of $n$, denoted $\delta^n$, satisfying
\begin{align}
  \|\theta-\theta_0\|_1 - \sqrt{\frac{4\log \frac{2}{\delta^n}}{n}}\geq \frac{1}{2} k_n,
\end{align}
then $E_n^{\delta^n} \cap J_n = \emptyset$ which simplies that $C_n^{\delta^n} \cap I_n = \emptyset$. Note that for small $n$ it might not be possible to find such a $\delta^n$, but $k_n$ decreases to zero as a function of $n$. For large enough $n$, therefore, we have that
\begin{align*}
  &\mathbb{P}[\hat{\theta} \in C_n^{\delta^n}] \geq 1-\delta^n\\
  &C_n^{\delta^n} \cap I_n = \emptyset\\
  \Rightarrow &\mathbb{P}[\hat{\theta}_n \in I_n] \leq \delta^n
\end{align*}
TODO: Need to work out the functional form of $\delta^n$ but the sketch of asymptotic power 1 goes like this. Demonstrate that
\begin{align*}
  \sum_{n=1}^{\infty}\mathbb{P}[\theta_n \in I_n] = c \leq \infty,
\end{align*}
then by the Borel Cantelli lemma
\begin{align*}
  \mathbb{P}[\hat{\theta}_n \in I_n i.o.] = 0,
\end{align*}
i.e. the test statistic cannot be in the indifference region infinitly many times.
\end{document}